{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://vk.com/u_samovaraa?w=wall-81871567_61842\n",
    "- https://vk.com/vkys_nos?w=wall-41960737_13333\n",
    "- https://vk.com/receptik_kulinar?w=wall-59496329_52708\n",
    "- https://vk.com/lisimnik_cake?w=wall-82240292_25648\n",
    "- https://vk.com/kingcook?w=wall-59442940_11047\n",
    "- https://vk.com/u_samovaraa?w=wall-81871567_61917\n",
    "- https://vk.com/quickrecipes?w=wall-61337543_5892\n",
    "- https://vk.com/namenuru?w=wall-36303114_56579\n",
    "- https://vk.com/vegan_cookbook?w=wall-43818640_25903\n",
    "- https://vk.com/multivarka_cook?w=wall-51300483_11948"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import iglob as list_paths\n",
    "\n",
    "\n",
    "def load_text(path):\n",
    "    with open(path) as file:\n",
    "        return file.read()\n",
    "\n",
    "\n",
    "texts = [\n",
    "    load_text(_)\n",
    "    for _ in list_paths('texts/*.txt')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_translation(source, target):\n",
    "    assert len(source) == len(target)\n",
    "    return {\n",
    "        ord(a): ord(b)\n",
    "        for a, b in zip(source, target)\n",
    "    }\n",
    "\n",
    "\n",
    "DASHES_TRANSLATION = make_translation(\n",
    "    '‚Äë‚Äì‚Äî‚àí',\n",
    "    '----'\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.replace('\\xa0', ' ')\n",
    "    text = text.replace('\\xad', '')\n",
    "    text = text.translate(DASHES_TRANSLATION)\n",
    "    return text\n",
    "\n",
    "\n",
    "texts = [preprocess(_) for _ in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°–∞–ª–∞—Ç —Å –∞–Ω–∞–Ω–∞—Å–∞–º–∏ –∏ –∫—É—Ä–∏—Ü–µ–π üçç\n",
      "\n",
      "–ò–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã:\n",
      "\n",
      "‚óè –§–∏–ª–µ –∫—É—Ä–∏–Ω–æ–π –≥—Ä—É–¥–∫–∏ 1 —à—Ç.\n",
      "‚óè –®–∞–º–ø–∏–Ω—å–æ–Ω—ã 300 –≥\n",
      "‚óè –ö—É–∫—É—Ä—É–∑–∞ –∫–æ–Ω—Å–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–Ω–∞—è 1 –±–∞–Ω–∫–∞\n",
      "‚óè –°—ã—Ä 200 –≥\n",
      "‚óè –õ—É–∫ —Ä–µ–ø—á–∞—Ç—ã–π 1 –≥–æ–ª–æ–≤–∫–∞\n",
      "‚óè –ß–µ—Å–Ω–æ–∫ 2 –∑—É–±—á–∏–∫–∞\n",
      "‚óè –ê–Ω–∞–Ω–∞—Å –∫–æ–Ω—Å–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–Ω—ã–π 1 –±–∞–Ω–∫–∞\n",
      "‚óè –ú–∞–π–æ–Ω–µ–∑ –ø–æ –≤–∫—É—Å—É\n",
      "\n",
      "–ü—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–µ:\n",
      "\n",
      "1. –û–±–∂–∞—Ä–∏—Ç—å –≥—Ä–∏–±—ã —Å –ª—É–∫–æ–º —Å –¥–æ–±–∞–≤–∏—Ç—å –Ω–µ–º–Ω–æ–≥–æ —Å–æ–ª–∏. –°—Ç–∞—Ä–∞–π—Ç–µ—Å—å –Ω–µ —Ä–µ–∑–∞—Ç—å –≥—Ä–∏–±—ã —Å–ª–∏—à–∫–æ–º –º–µ–ª–∫–æ.\n",
      "2. –û—Ç–≤–∞—Ä–∏—Ç—å –∫—É—Ä–∏–Ω—É—é –≥—Ä—É–¥–∫—É –±–µ–∑ –∫–æ–∂–∏, –Ω–∞—Ä–µ–∑–∞—Ç—å –∫—É—Å–æ—á–∫–∞–º–∏ –∏ –¥–æ–±–∞–≤–∏—Ç—å –∫ –Ω–µ–π –∫–æ–Ω—Å–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∫—É–∫—É—Ä—É–∑—É.\n",
      "3. –°—ã—Ä —Ç–∞–∫–∂–µ –Ω–∞—Ä–µ–∑–∞—Ç—å –º–∞–ª–µ–Ω—å–∫–∏–º–∏ –∫—É–±–∏–∫–∞–º–∏ –∏–ª–∏ –Ω–∞—Ç–µ—Ä–µ—Ç—å –Ω–∞ –∫—Ä—É–ø–Ω–æ–π —Ç–µ—Ä–∫–µ. –î–æ–±–∞–≤–∏—Ç—å –∫ –∫—É—Ä–∏—Ü–µ —Å –∫—É–∫—É—Ä—É–∑–æ–π.\n",
      "4. –°–º–µ—à–∞—Ç—å –≤—Å–µ –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã —Å –∫–æ–Ω—Å–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∞–Ω–∞–Ω–∞—Å–æ–º –∏ –∑–∞–ø—Ä–∞–≤–∏—Ç—å —Å–∞–ª–∞—Ç –º–∞–π–æ–Ω–µ–∑–æ–º —Å —á–µ—Å–Ω–æ–∫–æ–º.\n",
      "\n",
      "–ü—Ä–∏—è—Ç–Ω–æ–≥–æ –∞–ø–ø–µ—Ç–∏—Ç–∞!\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def find_ingredient_sections(text):\n",
    "    return re.findall(r'–∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã:(.+?)–ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–µ', text, re.I | re.S)\n",
    "\n",
    "\n",
    "\n",
    "def maybe_ingredient(line):\n",
    "    match = re.search(r'\\d', line)\n",
    "    size = len(line) <= 50\n",
    "    return match and size\n",
    "\n",
    "\n",
    "lines = []\n",
    "for text in texts:\n",
    "    sections = find_ingredient_sections(text)\n",
    "    for section in sections:\n",
    "        for line in section.splitlines():\n",
    "            if maybe_ingredient(line):\n",
    "                lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-1 –ª—É–∫–æ–≤–∏—Ü–∞ ',\n",
       " '1 —Å—Ç–∞–∫–∞–Ω –º—É–∫–∏',\n",
       " '2 —è–π—Ü–∞',\n",
       " '–°–æ–ª—å - 2 —Å—Ç.–ª–æ–∂–∫–∏',\n",
       " '-100 –≥—Ä–∞–º–º —Å—ã—Ä–∞ ',\n",
       " '–≤–∏—à–Ω–∏ –º–æ—Ä–æ–∂.3 –≥–æ—Ä—Å—Ç–∏,',\n",
       " '–ü–µ—Ä–µ—Ü –±–æ–ª–≥–∞—Ä—Å–∫–∏–π - 3 –®—Ç—É–∫–∏',\n",
       " '—Å–æ–∫ 1/2 –ª–∏–º–æ–Ω–∞ ',\n",
       " '1 —Ä–µ–ø—á–∞—Ç—ã–π –ª—É–∫ ',\n",
       " '—Å–æ–ª—å - 1 –ß–∞–π–Ω–∞—è –ª–æ–∂–∫–∞ (–¥–ª—è —Ç–µ—Å—Ç–∞) ',\n",
       " '‚óè –†–∞—Å—Ç–∏—Ç–µ–ª—å–Ω–æ–µ –º–∞—Å–ª–æ 1 —Å—Ç.–ª.',\n",
       " '–°—ã—Ä —Ç–≤–µ—Ä–¥—ã–π - 4 —Å—Ç. –ª.',\n",
       " '100 –≥ —Å–ª–∏–≤–æ—á–Ω–æ–≥–æ –º–∞—Å–ª–∞,',\n",
       " '1 –∑—É–±—á–∏–∫ —á–µ—Å–Ω–æ–∫–∞',\n",
       " '‚óè —Å–∞–ª—è–º–∏ 100 –≥ ',\n",
       " '-–û–≥—É—Ä–µ—Ü - 1 —à—Ç.',\n",
       " '—Ä–∞—Å—Ç–∏—Ç–µ–ª—å–Ω–æ–µ –º–∞—Å–ª–æ - 3 —Å—Ç. –ª–æ–∂–∫–∏ ',\n",
       " '4 —Å—Ä–µ–¥–Ω–∏—Ö —Å–ø–µ–ª—ã—Ö –±–∞–Ω–∞–Ω–∞',\n",
       " '–ö–∏–Ω–∑–∞ 2 –≥ ',\n",
       " '–†–µ–ø—á–∞—Ç—ã–π –ª—É–∫ - 2 –®—Ç—É–∫–∏ (—Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞)']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import seed, sample\n",
    "\n",
    "seed(1)\n",
    "sample(lines, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4054"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[MorphToken('—Ä—É–ª–µ—Ç–∏–∫–∏',\n",
       "            [0, 8),\n",
       "            'RU',\n",
       "            [Form('—Ä—É–ª–µ—Ç–∏–∫', Grams(NOUN,inan,masc,nomn,plur)),\n",
       "             Form('—Ä—É–ª–µ—Ç–∏–∫', Grams(NOUN,accs,inan,masc,plur)),\n",
       "             Form('—Ä—É–ª–µ—Ç–∏–∫', Grams(NOUN,inan,masc,nomn,plur)),\n",
       "             Form('—Ä—É–ª–µ—Ç–∏–∫', Grams(NOUN,accs,inan,masc,plur)),\n",
       "             Form('—Ä—É–ª—ë—Ç–∏–∫', Grams(NOUN,inan,masc,nomn,plur)),\n",
       "             Form('—Ä—É–ª—ë—Ç–∏–∫', Grams(NOUN,accs,inan,masc,plur))])]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from yargy.tokenizer import MorphTokenizer\n",
    "\n",
    "TOKENIZER = MorphTokenizer()\n",
    "list(TOKENIZER('—Ä—É–ª–µ—Ç–∏–∫–∏'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %run -n main.py\n",
    "# parser = Parser(INGREDIENT)\n",
    "# seed(1)\n",
    "# for line in sample(lines, 100):\n",
    "#     matches = list(parser.findall(line))\n",
    "#     spans = [_.span for _ in matches]\n",
    "#     show_markup(line, spans)\n",
    "# #     if matches:\n",
    "# #         match = matches[0]\n",
    "# #         display(match.tree.as_dot)\n",
    "# #         display(match.fact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
